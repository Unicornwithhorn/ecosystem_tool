from pathlib import Path
import pandas as pd

# =====================
# PATHS
# =====================
PROJECT_ROOT = Path(__file__).resolve().parents[1]

RAW_DIR = PROJECT_ROOT / "data" / "raw"
OUT_OBS = PROJECT_ROOT / "data" / "processed" / "observations.csv"
OUT_META = PROJECT_ROOT / "data" / "processed" / "descriptions.csv"
OUT_UNMATCHED = PROJECT_ROOT / "data" / "processed" / "unmatched_species.csv"
ALIASES_FILE = PROJECT_ROOT / "data" / "registry" / "species_aliases.csv"
ELLENBERG_XLSX = PROJECT_ROOT / "data" / "external" / "Indicator_values_Tichy_et_al.xlsx"

ELLENBERG_SHEET = "Tab-IVs-Tichy-et-al2022"

NBSP = "\u00A0"


# =====================
# HELPERS
# =====================

def normalize_text(s: pd.Series) -> pd.Series:
    return (
        s.astype("string")
        .str.replace(NBSP, " ", regex=False)
        .str.replace(r"\s+", " ", regex=True)
        .str.strip()
    )


def load_species_aliases(path: Path) -> dict[str, str]:
    if not path.exists():
        print("‚ÑπÔ∏è species_aliases.csv not found ‚Äî aliases not applied")
        return {}

    df = pd.read_csv(path, encoding="utf-8-sig", sep=None, engine="python")
    df.columns = normalize_text(df.columns)

    if not {"raw_species", "canonical_species"} <= set(df.columns):
        raise ValueError("species_aliases.csv must contain raw_species, canonical_species")

    df["raw_species"] = normalize_text(df["raw_species"])
    df["canonical_species"] = normalize_text(df["canonical_species"])

    df = df.dropna(subset=["raw_species", "canonical_species"])
    df = df.drop_duplicates(subset=["raw_species"], keep="last")

    print(f"‚ÑπÔ∏è Loaded {len(df)} species aliases")
    return dict(zip(df["raw_species"], df["canonical_species"]))


def apply_species_aliases(df: pd.DataFrame, species_col: str, aliases: dict) -> pd.DataFrame:
    out = df.copy()
    out["species_raw"] = normalize_text(out[species_col])
    out["species_canonical"] = out["species_raw"].map(aliases).fillna(out["species_raw"])
    return out


def load_ellenberg_species() -> set[str]:
    print(
        "ELLENBERG source:",
        ELLENBERG_XLSX.resolve(),
        "exists =",
        ELLENBERG_XLSX.exists()
    )

    if not ELLENBERG_XLSX.exists():
        print("‚ö†Ô∏è Ellenberg xlsx not found ‚Äî skipping check")
        return set()

    df = pd.read_excel(
        ELLENBERG_XLSX,
        sheet_name=ELLENBERG_SHEET,
        usecols=[1],   # –∫–æ–ª–æ–Ω–∫–∞ B
    )

    df.columns = ["species"]
    df["species"] = normalize_text(df["species"])

    species = df["species"].dropna().unique().tolist()
    print(f"‚ÑπÔ∏è Loaded {len(species)} Ellenberg species")
    return set(species)


# =====================
# MAIN PIPELINE
# =====================

def main() -> None:
    files = sorted(fp for fp in RAW_DIR.glob("*.xlsm") if not fp.name.startswith("~$"))

    if not files:
        raise RuntimeError("No .xlsm files found in data/raw")

    print(f"üìÇ Found {len(files)} raw files")

    aliases = load_species_aliases(ALIASES_FILE)
    ellenberg_species = load_ellenberg_species()

    obs_frames = []
    meta_frames = []

    for file in files:
        print(f"\n‚û° Processing: {file.name}")
        source = (
            str(file.stem)
            .replace("\u00A0", " ")
            .strip()
        )

        # -------- GEO-BOTANY --------
        df = pd.read_excel(file, sheet_name="–ì–µ–æ–±–æ—Ç–∞–Ω–∏–∫–∞")

        df = df.rename(columns={
            "–ò–Ω–¥–∏–≤–∏–¥u–∞–ª—å–Ω—ã–π ID –æ–ø–∏—Å–∞–Ω–∏—è": "description_id",
            "–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–∞": "species",
            "–í—ã—Å–æ—Ç–∞ (–º) –æ—Ç": "height_min",
            "–í—ã—Å–æ—Ç–∞ (–º) –¥–æ": "height_max",
            "–í—ã—Å–æ—Ç–∞ (–º) —Å—Ä–µ–¥": "height_mean",
            "–§e–Ωo—Ñ–∞–∑–∞": "phenophase",
            "–ñ–∏–∑–Ω–µ–Ω–Ω–æ—Å—Ç—å": "vitality",
            "–û–±–∏–ª–∏–µ": "abundance_class",
            "–ö–æ–ª-–≤–æ —Å—Ç–≤–æ–ª–æ–≤/ –∫—É—Å—Ç–æ–≤": "n_individuals",
        })

        df["description_id"] = df["description_id"].astype("Int64")
        df["species"] = normalize_text(df["species"])

        before = len(df)
        df = df[df["species"].notna() & (df["species"] != "#")]
        print(f"  removed empty species: {before - len(df)}")

        obs = df[
            [
                "description_id",
                "species",
                "height_min",
                "height_max",
                "height_mean",
                "phenophase",
                "vitality",
                "abundance_class",
                "n_individuals",
            ]
        ].copy()

        obs["source_file"] = source
        obs = apply_species_aliases(obs, "species", aliases)

        obs_frames.append(obs)

        # -------- METADATA --------
        meta_raw = pd.read_excel(file, sheet_name="–°–≤–æ–¥–Ω–∞—è")

        # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ ID –∏–∑ –≥–µ–æ–±–æ—Ç–∞–Ω–∏–∫–∏ (—Ç–æ, —Å —á–µ–º –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞—Å—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        obs_ids = set(pd.to_numeric(obs["description_id"], errors="coerce").astype("Int64").dropna().tolist())

        # –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è ID –≤ "–°–≤–æ–¥–Ω–æ–π" (—É —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –±—ã–≤–∞–µ—Ç –ø–æ-—Ä–∞–∑–Ω–æ–º—É)
        id_candidates = [
            "–ò–Ω–¥–∏–≤–∏–¥u–∞–ª—å–Ω—ã–π ID –æ–ø–∏—Å–∞–Ω–∏—è",
            "–ò–Ω–¥–∏–≤–∏–¥u–∞–ª—å–Ω—ã–π ID —Å—Ç—Ä–æ–∫–∏",
        ]

        # + –¥–æ–±–∞–≤–∏–º –∞–≤—Ç–æ-–ø–æ–∏—Å–∫: –ª—é–±—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –≥–¥–µ –µ—Å—Ç—å 'ID' –∏ ( '–æ–ø–∏—Å' –∏–ª–∏ '—Å—Ç—Ä–æ–∫' )
        auto_candidates = []
        for col in meta_raw.columns:
            col_s = str(col).lower()
            if "id" in col_s and ("–æ–ø–∏—Å" in col_s or "—Å—Ç—Ä–æ–∫" in col_s):
                auto_candidates.append(col)

        candidates = [c for c in id_candidates if c in meta_raw.columns] + auto_candidates

        best_col = None
        best_hits = -1

        for col in candidates:
            s = pd.to_numeric(meta_raw[col], errors="coerce").astype("Int64")
            hits = int(s.isin(list(obs_ids)).sum())
            if hits > best_hits:
                best_hits = hits
                best_col = col

        if best_col is None:
            raise ValueError(
                f"{file.name}: cannot find suitable ID column in '–°–≤–æ–¥–Ω–∞—è'. "
                f"Available columns: {list(meta_raw.columns)}"
            )

        meta = meta_raw.rename(columns={
            best_col: "description_id",
            "–ì–æ–¥": "year",
            "‚Ññ—Ç–æ—á–∫–∏ –Ω–∞ –ø—Ä–æ—Ñ–∏–ª–µ": "point_number",
            "–ü—Ä–æ—Ñ–∏–ª—å ‚Ññ": "cross_section_number",
            "–®–∏—Ä–æ—Ç–∞": "latitude",
            "–î–æ–ª–≥–æ—Ç–∞": "longitude",
            "–ì–µ–æ–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è": "geomorphology",
            "–î–æ–º–∏–Ω–∞–Ω—Ç –¥—Ä–µ–≤–µ—Å–Ω–æ–≥–æ —è—Ä—É—Å–∞": "tree_dominant",
            "0 –ª—É–≥ (–∫—Å–∫ –¥–æ 0,11), 1 —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π –ª–µ—Å (–¥–æ 0,21), 2 –ª–µ—Å (>=0,21) ": "afforestation",
            "–û–±—â–µ–µ –ø.–ø. (%)": "projective_cover",
            "–°–æ–º–∫–Ωu—Ç–æ—Å—Ç—å –∫—Ä–æ–Ω": "crown_density",
            "–í–µ–ª–∏—á–∏–Ω–∞ –ø–ª–æ—â–∞–¥–∫–∏ (–º2)": "description_area",
        })

        meta["description_id"] = pd.to_numeric(meta["description_id"], errors="coerce").astype("Int64")
        meta["point_number"] = pd.to_numeric(meta["point_number"], errors="coerce")
        meta["source_file"] = str(file.stem).replace("\u00A0", " ").strip()

        before_meta = len(meta)
        meta = meta.dropna(subset=["description_id"])
        # year ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ, –∏–Ω–∞—á–µ —ç—Ç–æ –Ω–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
        if "year" in meta.columns:
            meta = meta.dropna(subset=["year"])
        print(f"  meta rows kept: {len(meta)} (dropped {before_meta - len(meta)})")


        # --- QA: missing metadata for this file ---
        meta_ids = set(meta["description_id"].dropna().astype("Int64").tolist())
        missing_ids = sorted(obs_ids - meta_ids)
        missing_here = len(missing_ids)

        if missing_here:
            out_missing = (
                PROJECT_ROOT
                / "data"
                / "processed"
                / f"missing_meta_{source}.csv"
            )
            pd.DataFrame(
                {"description_id": missing_ids}
            ).to_csv(out_missing, index=False, encoding="utf-8")

            print(
                f"  ‚ö†Ô∏è metadata missing for {missing_here} descriptions "
                f"(ID col in '–°–≤–æ–¥–Ω–∞—è' = '{best_col}', hits={best_hits})"
            )
            print(f"  üßæ saved missing IDs list to: {out_missing}")
        else:
            print(
                f"  ‚úì metadata matched "
                f"(ID col in '–°–≤–æ–¥–Ω–∞—è' = '{best_col}', hits={best_hits})"
            )


        META_COLUMNS = [
            "description_id",
            "source_file",
            "year",
            "point_number",
            "cross_section_number",
            "latitude",
            "longitude",
            "geomorphology",
            "tree_dominant",
            "afforestation",
            "projective_cover",
            "crown_density",
            "description_area",
        ]
        keep = [c for c in META_COLUMNS if c in meta.columns]
        meta = meta[keep].copy()

        meta_frames.append(meta)

    # -------- CONCATENATE --------
    obs_all = pd.concat(obs_frames, ignore_index=True)
    meta_all = pd.concat(meta_frames, ignore_index=True)

    # -------- UNMATCHED --------
    if ellenberg_species:
        unmatched = (
            obs_all.loc[
                ~obs_all["species_canonical"].isin(ellenberg_species),
                ["species_raw", "species_canonical"],
            ]
            .drop_duplicates()
            .sort_values("species_raw")
        )
    else:
        unmatched = pd.DataFrame(columns=["species_raw", "species_canonical"])

    OUT_UNMATCHED.parent.mkdir(parents=True, exist_ok=True)
    unmatched.to_csv(OUT_UNMATCHED, index=False, encoding="utf-8")

    print(f"\n‚ö†Ô∏è Total unmatched species: {len(unmatched)}")

    # -------- SAVE --------
    OUT_OBS.parent.mkdir(parents=True, exist_ok=True)
    OUT_META.parent.mkdir(parents=True, exist_ok=True)

    obs_all.to_csv(OUT_OBS, index=False, encoding="utf-8")
    meta_all.to_csv(OUT_META, index=False, encoding="utf-8")

    print("\n‚úÖ DONE")
    print(f"Saved: {OUT_OBS}")
    print(f"Saved: {OUT_META}")
    print(f"Saved: {OUT_UNMATCHED}")


if __name__ == "__main__":
    main()
